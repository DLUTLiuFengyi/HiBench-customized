#### 11月4日

hibench.report 66行-236行记录

分别记录了tiny, small, large三档的用时
在如下配置情况下：

```conf
HiBench/conf spark.conf

# executor number and cores when running on Yarn
hibench.yarn.executor.num     16
hibench.yarn.executor.cores   4

# executor and driver memory in standalone & YARN mode
spark.executor.memory  16g
spark.driver.memory    16g

# set spark parallelism property according to hibench's parallelism value
spark.default.parallelism     ${hibench.default.map.parallelism}

# set spark sql's default shuffle partitions according to hibench's parallelism value
spark.sql.shuffle.partitions  ${hibench.default.shuffle.parallelism}

HiBench/conf hibench.conf

# The definition of these profiles can be found in the workload's conf file i.e. conf/workloads/micro/wordcount.conf
hibench.scale.profile              tiny

# Mapper number in hadoop, partition number in Spark
hibench.default.map.parallelism        16

# Reducer nubmer in hadoop, shuffle partition number in Spark
hibench.default.shuffle.parallelism     16
```

##### TIPS1

workload sleep 不读取数据，分区间不传递数据，只是简单的线程sleep，从benchmarks.lst中删去

workload aggregation Hadoop实现有错，删去

workload nutchindexing 只有Hadoop实现，且数据量在large档及之后执行出错，删去

##### TIPS2

large档，websearch/nutchindexing prepare failed

```log
Application application_1635858503466_0442 failed 2 times due to ApplicationMaster for attempt appattempt_1635858503466_0442_000002 timed out. Failing the application.
Exception in thread "main" java.io.IOException: Job failed!
```

#### 11月5日

增大并行度设置（注意不要超过yarn集群分配到的总资源数，否则报错）

```conf
HiBench/conf hibench.conf

# The definition of these profiles can be found in the workload's conf file i.e. conf/workloads/micro/wordcount.conf
hibench.scale.profile              tiny

# Mapper number in hadoop, partition number in Spark
hibench.default.map.parallelism        56

# Reducer nubmer in hadoop, shuffle partition number in Spark
hibench.default.shuffle.parallelism     32
```

tiny级别：用时与吞吐量没有显著变化，Hadoop有的workload用时增加5-10%，有的减少5-10%，Spark类似

| workload             | old    | new    |
| -------------------- | ------ | ------ |
| HadoopSort           | 19.73  | 23.8   |
| SparkSort            | 49.64  | 44.15  |
| HadoopTeraSort       | 21.07  | 24.88  |
| SparkTeraSort        | 45.11  | 41.87  |
| HadoopWordCount      | 20.79  | 25.04  |
| SparkWordCount       | 43.35  | 40.39  |
| HadoopDFS-read       | 38.11  | 38.54  |
| HadoopDFS-write      | 37.5   | 37.55  |
| HadoopJoin           | 20.54  | 13.89  |
| SparkJoin            | 71.05  | 69.04  |
| HadoopScan           | 12.38  | 11.55  |
| SparkScan            | 55.52  | 64.01  |
| HadoopPageRank       | 39.59  | 46.52  |
| SparkPageRank        | 46.93  | 46.77  |
| HadoopBayes          | 200.61 | 218.96 |
| SparkBayes           | 48.18  | 57.69  |
| HadoopKmeans         | 93.19  | 94.38  |
| SparkKmeans          | 53.64  | 51.84  |
| LogisticRegression   | 52.7   | 52.79  |
| ALS                  | 55.37  | 63.98  |
| PCA                  | 58.31  | 60.66  |
| GradientBoostingTree | 60.39  | 53.37  |
| RandomForest         | 53.21  | 45.81  |
| SVD                  | 52.97  | 44.21  |
| LinearRegression     | 105.46 | 109.69 |
| LDA                  | 62.61  | 63.83  |
| SVM                  | 56.25  | 59.37  |
| GaussianMixtureModel | 56.39  | 58.24  |
| SparkNWeight         | 46.42  | 47.9   |

237-323行是new配置，tiny、small、large三个级别的实验结果

变回old配置

##### 大数据量workload选择

* HadoopSort, SparkSort
* HadoopWordCount, SparkWordCount
* HadoopJoin, SparkJoin
* HadoopScan, SparkScan
* HadoopPageRank, SparkPageRank
* HadoopKmeans, SparkKmeans
* ALS
* PCA（gigantic档开始不行）
* GradientBoostingTree
* RandomForest
* SVD
* LDA




#### 11月6日

##### TIPS1

websearch与graph下各有一个pagerank，websearch下的pagerank有hadoop的实现

##### TIPS2

huge档，spark实现nweight，差不多一个小时后报错

```log
ShuffleMapStage 5 (mapPartitions at GraphImpl.scala:208) failed in 429.146 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 7

Exception in thread "main" org.apache.spark.SparkException: Job aborted
```

##### TIPS3

huge档，wordcount的prepare耗时65分钟

在自己家目录下搭建HiBench

* Hadoop：使用原hadoop

  ```
  hibench.hadoop.home    /home/cqx/software/hadoop-2.7.0
  hibench.hdfs.master       hdfs://amax7:9820/
  ```



#### 11月7日

##### TIPS1

* 数据量很大时，执行时间过长的workload（数据prepare的时间也很长），超过一个小时

  wordcount，pagerank

* 数据量很大时，执行时间相对不特别长的workload

  sort，kmeans，svd，lda（gigantic档40分钟）

* 数据量很大时，执行时间相对较短的workload

  join，scan，als，gbt，rf

##### TIPS2

gigantic档，spark执行PCA，在过程中出错

```log
21/11/07 20:56:57 ERROR cluster.YarnScheduler: Lost executor 5 on amax9: Container from a bad node: container_1635858503466_0807_01_000006 on host: amax9. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
...
21/11/07 20:58:24 ERROR client.TransportClient: Failed to send RPC RPC 5803428715140738332 to /10.176.24.161:46866: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
```

分析原因：Driver端内存不足

##### TIPS3

gigantic档，svm生成数据耗时40分钟

##### 在report中添加硬件参数等列属性信息

bash脚本的自定义函数体代码在workload_functions.sh

其中修改生成的hibench.report内容的函数是function gen_report()

列属性对应的变量名称映射在hibench_prop_env_mapping.py

在结果中添加硬件相关列属性内容步骤：

1. 在hibench_prop_env_mapping.py中添加列属性名称映射关系，大部分环境设置变量HiBench已给出

2. 在function gen_report()中进行添加

   注意要修改hibench.conf中的

   ```conf
   hibench.report.formats		"%-12s %-10s %-8s %-20s %-20s %-20s %-20s\n"
   ```

   同时 function get_field_name() 也要修改



* 添加硬件列属性

  * 初始：集群节点数（3），exe个数，单个exe核数，单个exe内存，driver内存，主频（固定），显存大小（固定），网络带宽（固定）

  * ```shell
    type,date,time,input_data_size,duration,throughput,throughput_per_node,node_num,processor_num,cpu_freq,free_memory,software,version,node_vcores,node_memory,map_parallelism,shuffle_parallelism,executor_num,cores_per_executor,memory_per_executor,memory_of_driver,memory_of_gpu,bandwidth
    任务名称，日期，时间，数据量，耗时，吞吐量，每节点吞吐量，节点数，处理器核数，cpu频数，节点内存，计算框架，计算框架版本，节点可用核数，节点可用内存，mapper/分区个数，reducer/shuffle分区个数，executor个数，每个executor核数，每个executor内存，driver内存，显存，网络带宽
    
    注：
    processor_num是每台节点的cpu核数，为命令 cat /proc/cpuinfo| grep "processor"| wc -l 获得的值；
    free_memory是每台节点的可用内存，为命令 free 获得的值；
    node_vcores是每台节点设定分配给yarn的cpu核数，由yarn-site.xml中的yarn.nodemanager.resource.cpu-vcores参数设置；
    node_memory是每台节点设定分配给yarn的可用内存，由yarn-site.xml中的yarn.nodemanager.resource.memory-mb参数设置。
    Hadoop、Spark、Flink存在yarn、executor的概念，而PyTorch、Tensorflow不存在这些概念，所以对于PyTorch、Tensorflow的样本，node_vcores=processor_num，node_memory=free_memory。
    ```
    
  * 查看cpu信息

    cat /proc/cpuinfo

    CPU总核数 = 物理CPU个数 * 每颗物理CPU的核数
    总逻辑CPU数 = 物理CPU个数 * 每颗物理CPU的核数 * 超线程数

    ```shell
    # 查看cpu信息（型号）
    cqx@amax9:~$ cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c
         64  Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz
    # 查看物理cpu个数
    cqx@amax9:~$ cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l
    2
    # 查看每个物理cpu中core的个数（即核数）
    cqx@amax9:~$ cat /proc/cpuinfo| grep "cpu cores"| uniq
    cpu cores	: 16
    # 查看逻辑cpu的个数
    cqx@amax9:~$ cat /proc/cpuinfo| grep "processor"| wc -l
    64
    ```

  * 查看显卡信息

    lspci  | grep -i vga  与  lspci -vs 03:00.0  与  nvidia-smi

    amax7, amax8, amax9 16m+128k （没有显示出英伟达显卡的信息，需要安装工具包才能使用nvidia-smi指令）

* tiny small huge 多跑几遍

* 小级别的数据量都改一下

* 硬件配置修改

